import torch
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import os
from utils.wafermap_filter import WaferMapFilter
from utils.density_aware_filter import DensityAwareWaferMapFilter
from utils.region_aware_filter import RegionAwareWaferMapFilter


def prepare_clean_data(data_configs, use_filter=True, filter_params=None, use_density_aware=False, use_region_aware=False):
    """
    ì—¬ëŸ¬ ì œí’ˆ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì™„ì „íˆ ì •ë¦¬ + í•„í„°ë§

    Args:
        data_configs: [
            {"path": "path1.npz", "name": "product1"},
            {"path": "path2.npz", "name": "mixed_products"},
        ]
        use_filter: í•„í„°ë§ ì ìš© ì—¬ë¶€
        filter_params: í•„í„° íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        use_density_aware: Trueë©´ ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„° ì‚¬ìš© (ê¶Œì¥!)

    Returns:
        clean_maps, clean_labels
    """

    print("="*60)
    mode_str = "ë°€ë„ ê¸°ë°˜ ì ì‘í˜•" if use_density_aware else "ì¼ë°˜"
    print(f"ğŸ§¹ ë°ì´í„° ì™„ì „ ì •ë¦¬ ì‹œì‘" + (f" ({mode_str} í•„í„°ë§ í¬í•¨)" if use_filter else ""))
    print("="*60)
    
    # í•„í„° ì´ˆê¸°í™”
    if use_filter:
        if use_density_aware:
            # ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„°
            filter_obj = DensityAwareWaferMapFilter()
        else:
            # ì¼ë°˜ í•„í„°
            if filter_params is None:
                filter_params = {
                    'min_component_size': 5,
                    'opening_kernel_size': 1,
                    'closing_kernel_size': 5,
                    'edge_preserve_strength': 0.9
                }
            filter_obj = WaferMapFilter(**filter_params)

    all_clean_maps = []
    all_clean_labels = []
    all_info = []

    for config in data_configs:
        file_path = config["path"]
        name = config.get("name", "unknown")

        print(f"\nğŸ“ {name} ë¡œë”© ì¤‘: {file_path}")

        if not os.path.exists(file_path):
            print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {file_path}")
            continue

        try:
            # ë°ì´í„° ë¡œë“œ
            data = np.load(file_path, allow_pickle=True)
            maps = data['maps']
            labels = data['ids'] if 'ids' in data else data['labels']

            print(f"   ì›ë³¸: {len(maps)}ê°œ")

            # ê°œë³„ ì •ë¦¬
            clean_maps = []
            clean_labels = []
            filtered_count = 0
            info_list = []

            for i, (wm, label) in enumerate(zip(maps, labels)):
                try:
                    # ì™„ì „í•œ ì •ë¦¬ ê³¼ì •
                    if isinstance(wm, np.ndarray) and wm.dtype == object:
                        # object array â†’ ê°•ì œ ë³€í™˜
                        clean_wm = np.array(wm.tolist(), dtype=np.float32)
                    elif isinstance(wm, np.ndarray):
                        clean_wm = wm.astype(np.float32)
                    elif isinstance(wm, (list, tuple)):
                        clean_wm = np.array(wm, dtype=np.float32)
                    else:
                        clean_wm = np.array(wm, dtype=np.float32)

                    # ê²€ì¦
                    if len(clean_wm.shape) != 2 or clean_wm.shape[0] == 0 or clean_wm.shape[1] == 0:
                        continue

                    # NaN, Inf ì²˜ë¦¬
                    if np.any(np.isnan(clean_wm)) or np.any(np.isinf(clean_wm)):
                        clean_wm = np.nan_to_num(clean_wm, nan=0.0, posinf=1.0, neginf=0.0)

                    # ì •ê·œí™”
                    if clean_wm.max() > 0:
                        clean_wm = clean_wm / clean_wm.max()

                    # ğŸ”¹ í•„í„°ë§ ì ìš©
                    if use_filter and clean_wm.sum() > 0:
                        original_defects = clean_wm.sum()
                        
                        if use_density_aware:
                            # ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„°ë§
                            clean_wm_org = clean_wm.copy()
                            clean_wm, info = filter_obj.filter_single_map(clean_wm)
                            filtered_defects = clean_wm.sum()
                            if info['strategy'] == "very_low" and use_region_aware:
                                region_obj = RegionAwareWaferMapFilter(n_sectors=12, n_rings=3, sector_density_threshold=0.1, closing_kernel_size=6, min_region_size=50)
                                clean_wm = region_obj.detect_clustering_regions(clean_wm.squeeze())
                                clean_wm = clean_wm.unsqueeze(dim=0)
                        else:
                            # ì¼ë°˜ í•„í„°ë§
                            clean_wm = filter_obj.filter_single_map(clean_wm)
                            filtered_defects = clean_wm.sum()
                            info = None
                        
                        # ë„ˆë¬´ ë§ì´ ì œê±°ë˜ë©´ ìŠ¤í‚µ (íŒ¨í„´ì´ ê±°ì˜ ì‚¬ë¼ì§) -> ì´ê²Œ ì§„ì§œ í•„ìš”í• ê¹Œ?
                        if filtered_defects < original_defects * 0.2:
                            clean_wm = clean_wm_org.copy()
                            label = label + "_filter"
                            # continue
                        
                        if filtered_defects < original_defects:
                            filtered_count += 1
                    # ìµœì¢… ê²€ì¦
                    assert isinstance(clean_wm, np.ndarray)
                    assert clean_wm.dtype == np.float32
                    assert len(clean_wm.shape) == 2
                    assert not np.any(np.isnan(clean_wm))

                    clean_maps.append(clean_wm)
                    clean_labels.append(label)
                    info_list.append(info)

                except Exception as e:
                    # ë¬¸ì œ ìˆëŠ” ë°ì´í„°ëŠ” ì¡°ìš©íˆ ê±´ë„ˆëœ€
                    continue

            success_rate = len(clean_maps) / len(maps) * 100
            print(f"   ì •ë¦¬ë¨: {len(clean_maps)}ê°œ ({success_rate:.1f}%)")
            if use_filter and filtered_count > 0:
                print(f"   í•„í„°ë§ë¨: {filtered_count}ê°œ ({filtered_count/len(clean_maps)*100:.1f}%)")

            all_clean_maps.extend(clean_maps)
            all_clean_labels.extend(clean_labels)
            all_info.extend(info_list)

        except Exception as e:
            print(f"âŒ {name} ë¡œë”© ì‹¤íŒ¨: {e}")
            continue

    print(f"\nâœ… ì „ì²´ ì •ë¦¬ ì™„ë£Œ: {len(all_clean_maps)}ê°œ")
    return all_clean_maps, all_clean_labels, all_info


class MultiSizeWaferDataset(Dataset):
    """ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” Dataset"""

    def __init__(self, wafer_maps, labels, target_size=(128, 128), 
                 use_filter=False, filter_on_the_fly=False, filter_params=None,
                 use_density_aware=False, is_training=False, use_augmentation=False):
        """
        Args:
            wafer_maps: ì›¨ì´í¼ë§µ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            target_size: ë¦¬ì‚¬ì´ì¦ˆ íƒ€ê²Ÿ í¬ê¸°
            use_filter: í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
            filter_on_the_fly: Trueë©´ __getitem__ ì‹œë§ˆë‹¤ í•„í„°ë§ (ëŠë¦¼, ë©”ëª¨ë¦¬ ì ˆì•½)
                              Falseë©´ ì´ˆê¸°í™” ì‹œ ëª¨ë‘ í•„í„°ë§ (ë¹ ë¦„, ë©”ëª¨ë¦¬ ì‚¬ìš©)
            filter_params: í•„í„° íŒŒë¼ë¯¸í„°
            use_density_aware: Trueë©´ ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„° ì‚¬ìš© (ê¶Œì¥!)
        """
        self.wafer_maps = []
        self.labels = []
        self.original_indices = []  # ğŸ”´ ì¶”ê°€!
        self.target_size = target_size
        self.use_filter = use_filter
        self.filter_on_the_fly = filter_on_the_fly
        self.use_density_aware = use_density_aware
        self.is_training = is_training
        self.use_augmentation = use_augmentation

        # í•„í„° ì´ˆê¸°í™”
        if use_filter:
            if use_density_aware:
                self.filter_obj = DensityAwareWaferMapFilter()
            else:
                if filter_params is None:
                    filter_params = {
                        'min_component_size': 5,
                        'opening_kernel_size': 1,
                        'closing_kernel_size': 5,
                        'edge_preserve_strength': 0.9
                    }
                self.filter_obj = WaferMapFilter(**filter_params)
        print(f"ğŸ›¡ï¸  Dataset ìƒì„± ì¤‘...")
        if use_filter and not filter_on_the_fly:
            print(f"   ì‚¬ì „ í•„í„°ë§ ì ìš© ì¤‘...")

        for idx, (wm, label) in enumerate(zip(wafer_maps, labels)):
            if (isinstance(wm, np.ndarray) and
                wm.dtype == np.float32 and
                len(wm.shape) == 2 and
                wm.shape[0] > 0 and wm.shape[1] > 0):

                # ì‚¬ì „ í•„í„°ë§ (filter_on_the_fly=Falseì¸ ê²½ìš°)
                if use_filter and not filter_on_the_fly:
                    if wm.sum() > 0:
                        original_defects = wm.sum()
                        
                        if use_density_aware:
                            wm, info = self.filter_obj.filter_single_map(wm)
                        else:
                            wm = self.filter_obj.filter_single_map(wm)
                        
                        # ë„ˆë¬´ ë§ì´ ì œê±°ë˜ë©´ ìŠ¤í‚µ
                        if wm.sum() < original_defects * 0.2:
                            continue
                self.wafer_maps.append(wm)
                self.labels.append(label)
                self.original_indices.append(idx)  # ğŸ”´ ì›ë³¸ ì¸ë±ìŠ¤ ì €ì¥

        print(f"   ìµœì¢… Dataset: {len(self.wafer_maps)}ê°œ")

    def __len__(self):
        return len(self.wafer_maps)

    def __getitem__(self, idx):
        wafer_map = self.wafer_maps[idx]
        label = self.labels[idx]
        original_idx = self.original_indices[idx]  # ğŸ”´ ì¶”ê°€

        # On-the-fly í•„í„°ë§ (filter_on_the_fly=Trueì¸ ê²½ìš°)
        if self.use_filter and self.filter_on_the_fly:
            if wafer_map.sum() > 0:
                if self.use_density_aware:
                    wafer_map, _ = self.filter_obj.filter_single_map(wafer_map)
                else:
                    wafer_map = self.filter_obj.filter_single_map(wafer_map)

        # ì•ˆì „í•œ ì „ì²˜ë¦¬
        tensor = torch.tensor(wafer_map, dtype=torch.float32)
        tensor_4d = tensor.unsqueeze(0).unsqueeze(0)
        resized = F.interpolate(tensor_4d, size=self.target_size, mode='bilinear', align_corners=False)
        resized = resized.squeeze(0)  # (1, H, W)

        # ğŸ”´ Augmentation ì ìš© (training ì‹œì—ë§Œ!)
        if self.is_training and self.use_augmentation:
            resized_aug = self._apply_augmentation(resized)
        else:
            resized_aug = None

        return resized, resized_aug, label, original_idx  # ğŸ”´ ì¸ë±ìŠ¤ë„ ë°˜í™˜
    
    def _apply_augmentation(self, tensor):
        """
        íšŒì „ ë¶ˆë³€ì„±ì„ ìœ„í•œ Augmentation
        D4 Dihedral groupì˜ 8ê°€ì§€ ë³€í™˜ ì¤‘ í•˜ë‚˜ë¥¼ ê· ë“±í•˜ê²Œ ì„ íƒ
        """
        # 8ê°€ì§€ ë³€í™˜ ì¤‘ í•˜ë‚˜ë¥¼ ê· ë“±í•˜ê²Œ ì„ íƒ
        transform_id = torch.randint(0, 8, (1,)).item()
        
        if transform_id == 0:
            return tensor  # Identity (ë³€í™˜ ì—†ìŒ)
        elif transform_id == 1:
            return torch.rot90(tensor, 1, dims=[1, 2])  # 90ë„ íšŒì „
        elif transform_id == 2:
            return torch.rot90(tensor, 2, dims=[1, 2])  # 180ë„ íšŒì „
        elif transform_id == 3:
            return torch.rot90(tensor, 3, dims=[1, 2])  # 270ë„ íšŒì „
        elif transform_id == 4:
            return torch.flip(tensor, dims=[2])  # ì¢Œìš° ë°˜ì „
        elif transform_id == 5:
            return torch.flip(tensor, dims=[1])  # ìƒí•˜ ë°˜ì „
        elif transform_id == 6:
            # 90ë„ íšŒì „ + ì¢Œìš° ë°˜ì „ (ëŒ€ê°ì„  ëŒ€ì¹­)
            return torch.flip(torch.rot90(tensor, 1, dims=[1, 2]), dims=[2])
        elif transform_id == 7:
            # 90ë„ íšŒì „ + ìƒí•˜ ë°˜ì „ (ë‹¤ë¥¸ ëŒ€ê°ì„  ëŒ€ì¹­)
            return torch.flip(torch.rot90(tensor, 1, dims=[1, 2]), dims=[1])


def collate_fn(batch):
    """ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•œ collate í•¨ìˆ˜
    + ë¬¸ì œ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ í•„í„°ë§í•˜ëŠ” collate í•¨ìˆ˜
    + ë¹ˆ ë§µ(ëª¨ë‘ 0)ë„ ì œê±°"""

    safe_data = []
    safe_data_aug = []
    safe_labels = []
    safe_indices = []  # ğŸ”´ ì¶”ê°€
    has_aug = True  # ğŸ”¹ ì²« ë²ˆì§¸ ìœ íš¨í•œ ìƒ˜í”Œë¡œ íŒë‹¨

    for data, data_aug, label, original_idx in batch:  # ğŸ”´ 4ê°œ ë°›ê¸°
        try:
            if (isinstance(data, torch.Tensor) and
                data.dtype == torch.float32 and
                len(data.shape) == 3 and
                data.sum() > 0):  # ë¹ˆ ë§µ ì œê±°:

                safe_data.append(data)
                safe_data_aug.append(data_aug)  # Noneì´ê±°ë‚˜ tensor
                safe_labels.append(label)
                safe_indices.append(original_idx)  # ğŸ”´ ì¶”ê°€

                # ğŸ”¹ ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ augmentation ì—¬ë¶€ íŒë‹¨
                if len(safe_data) == 1:
                    has_aug = (data_aug is not None)

        except:
            continue

    if len(safe_data) == 0:
        # ëª¨ë“  ìƒ˜í”Œì´ ë¬¸ì œì¸ ê²½ìš° ë”ë¯¸ ë°°ì¹˜ ë°˜í™˜
        dummy = torch.zeros((1, 1, 128, 128), dtype=torch.float32)
        return dummy, ["dummy"], [0]  # ğŸ”´ dummy index

    batch_data = torch.stack(safe_data)
    # ğŸ”¹ augmentationì´ ìˆìœ¼ë©´ stack, ì—†ìœ¼ë©´ None
    if has_aug:
        batch_data_aug = torch.stack(safe_data_aug)
    else:
        batch_data_aug = None

    return batch_data, batch_data_aug, safe_labels, safe_indices  # ğŸ”´ 4ê°œ ë°˜í™˜


# def create_dataloaders(wafer_maps, labels, batch_size=64, target_size=(128, 128), test_size=0.2, use_filter=True, filter_on_the_fly=False,
#                         filter_params=None, use_density_aware=False, is_training=False, use_augmentation=False):
#     """
#     í•„í„°ë§ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ì•ˆì „í•œ DataLoaderë“¤ ìƒì„±
    
#     Args:
#         wafer_maps: ì›¨ì´í¼ë§µ ë¦¬ìŠ¤íŠ¸
#         labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
#         batch_size: ë°°ì¹˜ í¬ê¸°
#         target_size: ë¦¬ì‚¬ì´ì¦ˆ íƒ€ê²Ÿ í¬ê¸°
#         test_size: validation ë¹„ìœ¨
#         use_filter: í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
#         filter_on_the_fly: Trueë©´ ëŸ°íƒ€ì„ í•„í„°ë§, Falseë©´ ì‚¬ì „ í•„í„°ë§
#         filter_params: í•„í„° íŒŒë¼ë¯¸í„°
#         use_density_aware: Trueë©´ ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„° ì‚¬ìš© (ê¶Œì¥!)
    
#     Returns:
#         train_loader, valid_loader
#     """

#     print("\nğŸ”§ ì•ˆì „í•œ DataLoader ìƒì„±")
#     print("="*40)

#     if use_filter:
#         if use_density_aware:
#             mode = "Density-Aware (ë°€ë„ ê¸°ë°˜ ì ì‘í˜•)"
#         else:
#             mode = "On-the-fly" if filter_on_the_fly else "Pre-filtering"
#         print(f"   í•„í„°ë§ ëª¨ë“œ: {mode}")
#     # Dataset ìƒì„±
#     dataset = MultiSizeWaferDataset(
#         wafer_maps, labels, 
#         target_size=target_size,
#         use_filter=use_filter,
#         filter_on_the_fly=filter_on_the_fly,
#         filter_params=filter_params,
#         use_density_aware=use_density_aware,
#         is_training=False,  # ê¸°ë³¸ê°’
#         use_augmentation=False
#     )

#     # train/valid ë¶„í• 
#     train_indices, valid_indices = train_test_split(
#         range(len(dataset)), test_size=test_size, random_state=42
#     )

#     # Train dataset with augmentation
#     # ğŸ”´ Train datasetì—ë§Œ augmentation í™œì„±í™”
#     train_dataset = torch.utils.data.Subset(dataset, train_indices)
#     train_dataset.dataset.is_training = True
#     train_dataset.dataset.use_augmentation = use_augmentation
    
#     # Valid dataset without augmentation
#     valid_dataset = torch.utils.data.Subset(dataset, valid_indices)
#     valid_dataset.dataset.is_training = False
#     valid_dataset.dataset.use_augmentation = False

#     print(f"   Train: {len(train_dataset)}ê°œ / Valid: {len(valid_dataset)}ê°œ")

#     # DataLoader ìƒì„±
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=batch_size,
#         shuffle=True,
#         collate_fn=collate_fn,
#         num_workers=0,
#         pin_memory=False,
#         drop_last=False
#     )

#     valid_loader = DataLoader(
#         valid_dataset,
#         batch_size=batch_size,
#         shuffle=False,
#         collate_fn=collate_fn,
#         num_workers=0,
#         pin_memory=False,
#         drop_last=False
#     )

#     print(f"   Train ë°°ì¹˜: {len(train_loader)}ê°œ / Valid ë°°ì¹˜: {len(valid_loader)}ê°œ")

#     return train_loader, valid_loader

def create_dataloaders(wafer_maps, labels, batch_size=64, target_size=(128, 128), test_size=0.2, 
                        use_filter=True, filter_on_the_fly=False, filter_params=None, 
                        use_density_aware=False, use_augmentation=False):
    
    print("\nğŸ”§ ì•ˆì „í•œ DataLoader ìƒì„±")
    print("="*40)

    if use_filter:
        if use_density_aware:
            mode = "Density-Aware (ë°€ë„ ê¸°ë°˜ ì ì‘í˜•)"
        elif filter_on_the_fly:
            mode = "On-the-fly"
    else:
        mode = "Pre-filtering"
    print(f"   í•„í„°ë§ ëª¨ë“œ: {mode}")

    # ğŸ”¹ train/valid ë¶„í• ì„ ë¨¼ì € ìˆ˜í–‰
    train_indices, valid_indices = train_test_split(
        range(len(wafer_maps)), test_size=test_size, random_state=42
    )
    print(len(train_indices), len(valid_indices))
    # ğŸ”¹ ë¶„í• ëœ ë°ì´í„°ë¡œ train/valid ë°ì´í„° ìƒì„±
    train_maps = [wafer_maps[i] for i in train_indices]
    train_labels = [labels[i] for i in train_indices]
    
    valid_maps = [wafer_maps[i] for i in valid_indices]
    valid_labels = [labels[i] for i in valid_indices]

    # ğŸ”¹ ë³„ë„ì˜ dataset ê°ì²´ ìƒì„±
    train_dataset = MultiSizeWaferDataset(
        train_maps, train_labels, 
        target_size=target_size,
        use_filter=use_filter,
        filter_on_the_fly=filter_on_the_fly,
        filter_params=filter_params,
        use_density_aware=use_density_aware,
        is_training=True,  # ğŸ”¹ trainì€ True
        use_augmentation=use_augmentation
    )
    
    valid_dataset = MultiSizeWaferDataset(
        valid_maps, valid_labels, 
        target_size=target_size,
        use_filter=use_filter,
        filter_on_the_fly=filter_on_the_fly,
        filter_params=filter_params,
        use_density_aware=use_density_aware,
        is_training=False,  # ğŸ”¹ validì€ False
        use_augmentation=False  # ğŸ”¹ í•­ìƒ False
    )

    print(f"   Train: {len(train_dataset)}ê°œ (Augmentation: {use_augmentation})")
    print(f"   Valid: {len(valid_dataset)}ê°œ (Augmentation: False Fixed)")

    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=False,
        drop_last=False
    )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=False,
        drop_last=False
    )

    print(f"   Train ë°°ì¹˜: {len(train_loader)}ê°œ / Valid ë°°ì¹˜: {len(valid_loader)}ê°œ")

    return train_loader, valid_loader