import torch
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import os
from utils.wafermap_filter import WaferMapFilter
from utils.density_aware_filter import DensityAwareWaferMapFilter
from utils.region_aware_filter import RegionAwareWaferMapFilter


def convert_to_multichannel(wafer_map, n_categories=10):
    """
    Convert wafer map to multi-channel format
    
    Args:
        wafer_map: numpy array
                   - Binary: (H, W) with values {0, 1}
                   - Category: (n_cat, H, W) with values {0, 1} for each category
        n_categories: number of categories (default: 10)
    
    Returns:
        multi_channel: (n_categories+1, H, W) numpy array
                      channel[0] = binary (spatial pattern)
                      channel[1:n+1] = category-specific
    """
    # Detect input format
    if len(wafer_map.shape) == 2:
        # Binary data: (H, W)
        H, W = wafer_map.shape
        multi_channel = np.zeros((n_categories + 1, H, W), dtype=np.float32)
        
        # Channel 0: binary map
        multi_channel[0] = wafer_map.astype(np.float32)
        
        # Channel 1-n: all zeros (no category info)
        # Already initialized as zeros
        
    elif len(wafer_map.shape) == 3:
        # Category data: (n_cat, H, W)
        n_cat, H, W = wafer_map.shape
        multi_channel = np.zeros((n_categories + 1, H, W), dtype=np.float32)
        
        # Channel 0: binary (any category > 0)
        multi_channel[0] = (wafer_map.sum(axis=0) > 0).astype(np.float32)
        
        # Channel 1-n: category-specific
        # Copy existing categories
        multi_channel[1:n_cat+1] = wafer_map.astype(np.float32)
        
        # If n_cat < n_categories, remaining channels stay 0
        
    else:
        raise ValueError(f"Unexpected wafer_map shape: {wafer_map.shape}")
    
    return multi_channel

def detect_n_categories(data_configs):
    """
    Automatically detect maximum number of categories from data
    """
    max_categories = 0
    
    for config in data_configs:
        file_path = config["path"]
        
        if not os.path.exists(file_path):
            continue
        
        try:
            data = np.load(file_path, allow_pickle=True)
            maps = data['maps']
            
            # Check first sample
            if len(maps) > 0:
                sample = maps[0]
                
                # Convert to array if needed
                if not isinstance(sample, np.ndarray):
                    sample = np.array(sample)
                
                # Category data: 3D (n_cat, H, W)
                if len(sample.shape) == 3:
                    n_cat = sample.shape[0]
                    max_categories = max(max_categories, n_cat)
                # Binary data: 2D (H, W)
                elif len(sample.shape) == 2:
                    pass  # Binary, no categories
                    
        except Exception as e:
            print(f"âš ï¸  Failed to detect categories from {file_path}: {e}")
            continue
    
    print(f"âœ… Detected maximum categories: {max_categories}")
    return max_categories



def prepare_clean_data(data_configs, use_filter=True, filter_params=None, 
                       use_density_aware=False, use_region_aware=False):
    """
    ì—¬ëŸ¬ ì œí’ˆ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì™„ì „íˆ ì •ë¦¬ + í•„í„°ë§ + Multi-channel ë³€í™˜

    Args:
        data_configs: [
            {"path": "path1.npz", "name": "product1"},
            {"path": "path2.npz", "name": "mixed_products"},
        ]
        use_filter: í•„í„°ë§ ì ìš© ì—¬ë¶€
        filter_params: í•„í„° íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        use_density_aware: Trueë©´ ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„° ì‚¬ìš© (ê¶Œì¥!)
        use_region_aware: Trueë©´ region-aware í•„í„° ì‚¬ìš©

    Returns:
        clean_maps: List of (n_categories+1, H, W) arrays
        clean_labels: List of labels
        info: List of filter info dicts
    """

    print("="*60)
    
    # 1. ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ ìë™ ê°ì§€
    n_categories = detect_n_categories(data_configs)
    print(f"ğŸ” Auto-detected categories: {n_categories}")
    
    mode_str = "ë°€ë„ ê¸°ë°˜ ì ì‘í˜•" if use_density_aware else "ì¼ë°˜"
    print(f"ğŸ§¹ ë°ì´í„° ì™„ì „ ì •ë¦¬ ì‹œì‘" + (f" ({mode_str} í•„í„°ë§ í¬í•¨)" if use_filter else ""))
    print(f"ğŸ“Š Multi-channel format: {n_categories + 1} channels")
    print("="*60)
    
    # í•„í„° ì´ˆê¸°í™”
    if use_filter:
        if use_density_aware:
            filter_obj = DensityAwareWaferMapFilter()
        else:
            if filter_params is None:
                filter_params = {
                    'min_component_size': 5,
                    'opening_kernel_size': 1,
                    'closing_kernel_size': 5,
                    'edge_preserve_strength': 0.9
                }
            filter_obj = WaferMapFilter(**filter_params)

    all_clean_maps = []
    all_clean_labels = []
    all_info = []

    for config in data_configs:
        file_path = config["path"]
        name = config.get("name", "unknown")

        print(f"\nğŸ“ {name} ë¡œë”© ì¤‘: {file_path}")

        if not os.path.exists(file_path):
            print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {file_path}")
            continue

        try:
            # ë°ì´í„° ë¡œë“œ
            data = np.load(file_path, allow_pickle=True)
            maps = data['maps']
            labels = data['ids'] if 'ids' in data else data['labels']

            print(f"   ì›ë³¸: {len(maps)}ê°œ")

            # ê°œë³„ ì •ë¦¬
            clean_maps = []
            clean_labels = []
            filtered_count = 0
            info_list = []

            for i, (wm, label) in enumerate(zip(maps, labels)):
                try:
                    # wmì„ numpy arrayë¡œ ë³€í™˜
                    if not isinstance(wm, np.ndarray):
                        wm = np.array(wm.tolist() if hasattr(wm, 'tolist') else wm, dtype=np.float32)
                    elif wm.dtype == object:
                        wm = np.array(wm.tolist(), dtype=np.float32)
                    else:
                        wm = wm.astype(np.float32)
                    
                    # ê²€ì¦: 2D (binary) ë˜ëŠ” 3D (category)
                    if len(wm.shape) == 2:
                        # Binary data (H, W)
                        H, W = wm.shape
                        if H == 0 or W == 0:
                            continue
                        
                    elif len(wm.shape) == 3:
                        # Category data (n_cat, H, W)
                        n_cat, H, W = wm.shape
                        if n_cat == 0 or H == 0 or W == 0:
                            continue
                    else:
                        # Invalid shape
                        continue
                    
                    # NaN, Inf ì²˜ë¦¬
                    if np.any(np.isnan(wm)) or np.any(np.isinf(wm)):
                        wm = np.nan_to_num(wm, nan=0.0, posinf=1.0, neginf=0.0)

                    # ì •ê·œí™” (0-1 range)
                    if wm.max() > 1.0:
                        wm = wm / wm.max()
                    
                    # ğŸ”¹ Multi-channel ë³€í™˜
                    multi_channel_wm = convert_to_multichannel(wm, n_categories)
                    
                    # ğŸ”¹ í•„í„°ë§ ì ìš© (channel 0ì—ë§Œ)
                    if use_filter and multi_channel_wm[0].sum() > 0:
                        original_defects = multi_channel_wm[0].sum()
                        
                        if use_density_aware:
                            filtered_ch0, info = filter_obj.filter_single_map(multi_channel_wm[0])
                            multi_channel_wm[0] = filtered_ch0
                        else:
                            multi_channel_wm[0] = filter_obj.filter_single_map(multi_channel_wm[0])
                            info = None
                        
                        filtered_defects = multi_channel_wm[0].sum()
                        
                        # ë„ˆë¬´ ë§ì´ ì œê±°ë˜ë©´ ìŠ¤í‚µ
                        if filtered_defects < original_defects * 0.2:
                            continue
                        
                        if filtered_defects < original_defects:
                            filtered_count += 1
                    
                    # ìµœì¢… ê²€ì¦
                    assert isinstance(multi_channel_wm, np.ndarray)
                    assert multi_channel_wm.dtype == np.float32
                    assert len(multi_channel_wm.shape) == 3  # (C, H, W)
                    assert multi_channel_wm.shape[0] == n_categories + 1
                    assert not np.any(np.isnan(multi_channel_wm))

                    clean_maps.append(multi_channel_wm)
                    clean_labels.append(label)
                    info_list.append(info)

                except Exception as e:
                    # ë¬¸ì œ ìˆëŠ” ë°ì´í„°ëŠ” ì¡°ìš©íˆ ê±´ë„ˆëœ€
                    continue

            success_rate = len(clean_maps) / len(maps) * 100
            print(f"   ì •ë¦¬ë¨: {len(clean_maps)}ê°œ ({success_rate:.1f}%)")
            print(f"   Shape: ({n_categories + 1}, H, W)")
            if use_filter and filtered_count > 0:
                print(f"   í•„í„°ë§ë¨: {filtered_count}ê°œ ({filtered_count/len(clean_maps)*100:.1f}%)")

            all_clean_maps.extend(clean_maps)
            all_clean_labels.extend(clean_labels)
            all_info.extend(info_list)

        except Exception as e:
            print(f"âŒ {name} ë¡œë”© ì‹¤íŒ¨: {e}")
            continue

    print(f"\nâœ… ì „ì²´ ì •ë¦¬ ì™„ë£Œ: {len(all_clean_maps)}ê°œ")
    print(f"   Final shape per sample: ({n_categories + 1}, H, W)")
    return all_clean_maps, all_clean_labels, all_info


class MultiSizeWaferDataset(Dataset):
    """ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” Dataset"""

    def __init__(self, wafer_maps, labels, target_size=(128, 128), 
                 use_filter=False, filter_on_the_fly=False, filter_params=None,
                 use_density_aware=False, is_training=False, use_augmentation=False):
        """
        Args:
            wafer_maps: ì›¨ì´í¼ë§µ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            target_size: ë¦¬ì‚¬ì´ì¦ˆ íƒ€ê²Ÿ í¬ê¸°
            use_filter: í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
            filter_on_the_fly: Trueë©´ __getitem__ ì‹œë§ˆë‹¤ í•„í„°ë§ (ëŠë¦¼, ë©”ëª¨ë¦¬ ì ˆì•½)
                              Falseë©´ ì´ˆê¸°í™” ì‹œ ëª¨ë‘ í•„í„°ë§ (ë¹ ë¦„, ë©”ëª¨ë¦¬ ì‚¬ìš©)
            filter_params: í•„í„° íŒŒë¼ë¯¸í„°
            use_density_aware: Trueë©´ ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„° ì‚¬ìš© (ê¶Œì¥!)
        """
        self.wafer_maps = []
        self.labels = []
        self.original_indices = []
        self.target_size = target_size
        self.use_filter = use_filter
        self.filter_on_the_fly = filter_on_the_fly
        self.use_density_aware = use_density_aware
        self.is_training = is_training
        self.use_augmentation = use_augmentation

        # í•„í„° ì´ˆê¸°í™”
        if use_filter:
            if use_density_aware:
                self.filter_obj = DensityAwareWaferMapFilter()
            else:
                if filter_params is None:
                    filter_params = {
                        'min_component_size': 5,
                        'opening_kernel_size': 1,
                        'closing_kernel_size': 5,
                        'edge_preserve_strength': 0.9
                    }
                self.filter_obj = WaferMapFilter(**filter_params)
        
        print(f"ğŸ›¡ï¸  Dataset ìƒì„± ì¤‘...")
        if use_filter and not filter_on_the_fly:
            print(f"   ì‚¬ì „ í•„í„°ë§ ì ìš© ì¤‘...")

        for idx, (wm, label) in enumerate(zip(wafer_maps, labels)):
            # ğŸ”´ Shape ê²€ì¦ ìˆ˜ì •: (C, H, W) í˜•ì‹
            if (isinstance(wm, np.ndarray) and
                wm.dtype == np.float32 and
                len(wm.shape) == 3 and      # (C, H, W)
                wm.shape[0] > 0 and         # C > 0
                wm.shape[1] > 0 and         # H > 0
                wm.shape[2] > 0):           # W > 0

                # ì‚¬ì „ í•„í„°ë§ (filter_on_the_fly=Falseì¸ ê²½ìš°)
                # Channel 0ì—ë§Œ ì ìš©
                if use_filter and not filter_on_the_fly:
                    if wm[0].sum() > 0:
                        original_defects = wm[0].sum()
                        
                        if use_density_aware:
                            wm[0], info = self.filter_obj.filter_single_map(wm[0])
                        else:
                            wm[0] = self.filter_obj.filter_single_map(wm[0])
                        
                        # ë„ˆë¬´ ë§ì´ ì œê±°ë˜ë©´ ìŠ¤í‚µ
                        if wm[0].sum() < original_defects * 0.2:
                            continue
                
                self.wafer_maps.append(wm)
                self.labels.append(label)
                self.original_indices.append(idx)

        print(f"   ìµœì¢… Dataset: {len(self.wafer_maps)}ê°œ")

    def __len__(self):
        return len(self.wafer_maps)

    def __getitem__(self, idx):
        wafer_map = self.wafer_maps[idx]  # (C, H, W) - already multi-channel
        label = self.labels[idx]
        original_idx = self.original_indices[idx]

        # On-the-fly í•„í„°ë§ (filter_on_the_fly=Trueì¸ ê²½ìš°)
        # Note: ì´ë¯¸ prepare_clean_dataì—ì„œ í•„í„°ë§ í–ˆìœ¼ë¯€ë¡œ ë³´í†µì€ skip
        if self.use_filter and self.filter_on_the_fly:
            # Channel 0ì—ë§Œ í•„í„°ë§ ì ìš©
            if wafer_map[0].sum() > 0:
                if self.use_density_aware:
                    wafer_map[0], _ = self.filter_obj.filter_single_map(wafer_map[0])
                else:
                    wafer_map[0] = self.filter_obj.filter_single_map(wafer_map[0])

        # ì•ˆì „í•œ ì „ì²˜ë¦¬
        # wafer_map: (C, H, W) numpy array â†’ (C, target_H, target_W) tensor
        tensor = torch.tensor(wafer_map, dtype=torch.float32)  # (C, H, W)
        
        # Resize
        # F.interpolate expects (B, C, H, W), so add batch dim
        tensor_4d = tensor.unsqueeze(0)  # (1, C, H, W)
        resized = F.interpolate(tensor_4d, size=self.target_size, mode='bilinear', align_corners=False)
        resized = resized.squeeze(0)  # (C, target_H, target_W)

        # ğŸ”´ Augmentation ì ìš© (training ì‹œì—ë§Œ!)
        if self.is_training and self.use_augmentation:
            resized_aug = self._apply_augmentation(resized)
        else:
            resized_aug = None

        return resized, resized_aug, label, original_idx
    
    def _apply_augmentation(self, tensor):
        """
        íšŒì „ ë¶ˆë³€ì„±ì„ ìœ„í•œ Augmentation
        D4 Dihedral groupì˜ 8ê°€ì§€ ë³€í™˜ ì¤‘ í•˜ë‚˜ë¥¼ ê· ë“±í•˜ê²Œ ì„ íƒ
        
        Args:
            tensor: (C, H, W) - multi-channel
        
        Returns:
            tensor: (C, H, W) - augmented
        """
        # 8ê°€ì§€ ë³€í™˜ ì¤‘ í•˜ë‚˜ë¥¼ ê· ë“±í•˜ê²Œ ì„ íƒ
        transform_id = torch.randint(0, 8, (1,)).item()
        
        if transform_id == 0:
            return tensor  # Identity (ë³€í™˜ ì—†ìŒ)
        elif transform_id == 1:
            return torch.rot90(tensor, 1, dims=[1, 2])  # 90ë„ íšŒì „
        elif transform_id == 2:
            return torch.rot90(tensor, 2, dims=[1, 2])  # 180ë„ íšŒì „
        elif transform_id == 3:
            return torch.rot90(tensor, 3, dims=[1, 2])  # 270ë„ íšŒì „
        elif transform_id == 4:
            return torch.flip(tensor, dims=[2])  # ì¢Œìš° ë°˜ì „
        elif transform_id == 5:
            return torch.flip(tensor, dims=[1])  # ìƒí•˜ ë°˜ì „
        elif transform_id == 6:
            # 90ë„ íšŒì „ + ì¢Œìš° ë°˜ì „ (ëŒ€ê°ì„  ëŒ€ì¹­)
            return torch.flip(torch.rot90(tensor, 1, dims=[1, 2]), dims=[2])
        elif transform_id == 7:
            # 90ë„ íšŒì „ + ìƒí•˜ ë°˜ì „ (ë‹¤ë¥¸ ëŒ€ê°ì„  ëŒ€ì¹­)
            return torch.flip(torch.rot90(tensor, 1, dims=[1, 2]), dims=[1])


def collate_fn(batch):
    """ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•œ collate í•¨ìˆ˜
    + ë¬¸ì œ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ í•„í„°ë§í•˜ëŠ” collate í•¨ìˆ˜
    + ë¹ˆ ë§µ(ëª¨ë‘ 0)ë„ ì œê±°
    + Multi-channel ì§€ì›"""

    safe_data = []
    safe_data_aug = []
    safe_labels = []
    safe_indices = []
    has_aug = True

    for data, data_aug, label, original_idx in batch:
        try:
            # Shape ê²€ì¦: (C, H, W)
            if (isinstance(data, torch.Tensor) and
                data.dtype == torch.float32 and
                len(data.shape) == 3 and  # (C, H, W)
                data.shape[0] > 0 and     # C > 0
                data.shape[1] > 0 and     # H > 0
                data.shape[2] > 0 and     # W > 0
                data.sum() > 0):          # Not all zeros

                safe_data.append(data)
                safe_data_aug.append(data_aug)
                safe_labels.append(label)
                safe_indices.append(original_idx)

                # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ augmentation ì—¬ë¶€ íŒë‹¨
                if len(safe_data) == 1:
                    has_aug = (data_aug is not None)

        except:
            continue

    if len(safe_data) == 0:
        # ëª¨ë“  ìƒ˜í”Œì´ ë¬¸ì œì¸ ê²½ìš° ë”ë¯¸ ë°°ì¹˜ ë°˜í™˜
        # Multi-channel dummy
        dummy = torch.zeros((1, 11, 128, 128), dtype=torch.float32)  # 11 channels
        return dummy, None, ["dummy"], [0]

    batch_data = torch.stack(safe_data)
    
    if has_aug:
        batch_data_aug = torch.stack(safe_data_aug)
    else:
        batch_data_aug = None

    return batch_data, batch_data_aug, safe_labels, safe_indices



def create_dataloaders(wafer_maps, labels, batch_size=64, target_size=(128, 128), test_size=0.2, 
                        use_filter=True, filter_on_the_fly=False, filter_params=None, 
                        use_density_aware=False, use_augmentation=False):
    
    print("\nğŸ”§ ì•ˆì „í•œ DataLoader ìƒì„±")
    print("="*40)

    if use_filter:
        if use_density_aware:
            mode = "Density-Aware (ë°€ë„ ê¸°ë°˜ ì ì‘í˜•)"
        elif filter_on_the_fly:
            mode = "On-the-fly"
    else:
        mode = "Pre-filtering"
    print(f"   í•„í„°ë§ ëª¨ë“œ: {mode}")

    # ğŸ”¹ train/valid ë¶„í• ì„ ë¨¼ì € ìˆ˜í–‰
    train_indices, valid_indices = train_test_split(
        range(len(wafer_maps)), test_size=test_size, random_state=42
    )
    
    # ğŸ”¹ ë¶„í• ëœ ë°ì´í„°ë¡œ train/valid ë°ì´í„° ìƒì„±
    train_maps = [wafer_maps[i] for i in train_indices]
    train_labels = [labels[i] for i in train_indices]
    
    valid_maps = [wafer_maps[i] for i in valid_indices]
    valid_labels = [labels[i] for i in valid_indices]

    # ğŸ”¹ ë³„ë„ì˜ dataset ê°ì²´ ìƒì„±
    train_dataset = MultiSizeWaferDataset(
        train_maps, train_labels, 
        target_size=target_size,
        use_filter=use_filter,
        filter_on_the_fly=filter_on_the_fly,
        filter_params=filter_params,
        use_density_aware=use_density_aware,
        is_training=True,  # ğŸ”¹ trainì€ True
        use_augmentation=use_augmentation
    )
    
    valid_dataset = MultiSizeWaferDataset(
        valid_maps, valid_labels, 
        target_size=target_size,
        use_filter=use_filter,
        filter_on_the_fly=filter_on_the_fly,
        filter_params=filter_params,
        use_density_aware=use_density_aware,
        is_training=False,  # ğŸ”¹ validì€ False
        use_augmentation=False  # ğŸ”¹ í•­ìƒ False
    )

    print(f"   Train: {len(train_dataset)}ê°œ (Augmentation: {use_augmentation})")
    print(f"   Valid: {len(valid_dataset)}ê°œ (Augmentation: False Fixed)")

    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=False,
        drop_last=False
    )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=False,
        drop_last=False
    )

    print(f"   Train ë°°ì¹˜: {len(train_loader)}ê°œ / Valid ë°°ì¹˜: {len(valid_loader)}ê°œ")

    return train_loader, valid_loader