import torch
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import os
from utils.wafermap_filter import WaferMapFilter
from utils.density_aware_filter import DensityAwareWaferMapFilter
from utils.region_aware_filter import RegionAwareWaferMapFilter


def convert_to_multichannel(wafer_maps, n_categories=12):
    """
    ì •ìˆ˜ ë¼ë²¨ ë°°ì—´ì„ multi-channel one-hot í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë°°ì¹˜ ë²¡í„°í™”)
    
    Args:
        wafer_maps: numpy array
                   - ë‹¨ì¼: (H, W) ì •ìˆ˜ ë°°ì—´ (ê°’ 0~12)
                   - ë°°ì¹˜: (n, H, W) ì •ìˆ˜ ë°°ì—´ (ê°’ 0~12)
        n_categories: ë¶ˆëŸ‰ ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 12)
                     0ì€ non-wafer + good chipì´ë¯€ë¡œ ì œì™¸
    
    Returns:
        multi_channel: numpy array
                      - ë‹¨ì¼ ì…ë ¥: (n_categories+1, H, W)
                      - ë°°ì¹˜ ì…ë ¥: (n, n_categories+1, H, W)
                      
                      channel[0] = ë¶ˆëŸ‰ ìœ„ì¹˜ (ê°’ > 0)
                      channel[1~12] = ê° ì¹´í…Œê³ ë¦¬ë³„ ìœ„ì¹˜ (ê°’ == k)
    """
    # ë‹¨ì¼ ì›¨ì´í¼ ì²˜ë¦¬ (H, W) â†’ ë°°ì¹˜ í˜•íƒœë¡œ ë³€í™˜
    single_input = False
    if len(wafer_maps.shape) == 2:
        single_input = True
        wafer_maps = wafer_maps[np.newaxis, ...]  # (1, H, W)
    
    n, H, W = wafer_maps.shape
    n_channels = n_categories + 1  # 13 channels (0: spatial, 1-12: categories)
    
    # ê²°ê³¼ ë°°ì—´ ì´ˆê¸°í™”
    multi_channel = np.zeros((n, n_channels, H, W), dtype=np.float32)
    
    # Channel 0: ë¶ˆëŸ‰ ìœ„ì¹˜ (ê°’ > 0ì¸ ëª¨ë“  ìœ„ì¹˜)
    multi_channel[:, 0, :, :] = (wafer_maps > 0).astype(np.float32)
    
    # Channel 1~12: ê° ì¹´í…Œê³ ë¦¬ë³„ one-hot
    for k in range(1, n_categories + 1):
        multi_channel[:, k, :, :] = (wafer_maps == k).astype(np.float32)
    
    # ë‹¨ì¼ ì…ë ¥ì´ì—ˆìœ¼ë©´ ë°°ì¹˜ ì°¨ì› ì œê±°
    if single_input:
        multi_channel = multi_channel[0]  # (13, H, W)
    
    return multi_channel

def detect_n_categories(data_configs):
    """
    ë°ì´í„° íŒŒì¼ë“¤ì„ ìŠ¤ìº”í•˜ì—¬ ìµœëŒ€ ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ ìë™ ê°ì§€ (ê°’ ê¸°ë°˜)
    
    Args:
        data_configs: [{"path": "...", "name": "..."}, ...]
    
    Returns:
        max_category: ìµœëŒ€ ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸ (ì˜ˆ: 12)
                     0ì€ non-wafer + good chipì´ë¯€ë¡œ ì¹´í…Œê³ ë¦¬ì—ì„œ ì œì™¸
    """
    max_category = 0
    
    for config in data_configs:
        file_path = config["path"]
        
        if not os.path.exists(file_path):
            print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {file_path}")
            continue
        
        try:
            data = np.load(file_path, allow_pickle=True)
            maps = data['maps']
            
            if len(maps) == 0:
                continue
            
            # ìƒ˜í”Œë§í•˜ì—¬ ìµœëŒ€ê°’ í™•ì¸ (ì „ì²´ ìŠ¤ìº”ì€ ëŠë¦´ ìˆ˜ ìˆìŒ)
            n_samples = min(100, len(maps))
            sample_indices = np.linspace(0, len(maps)-1, n_samples, dtype=int)
            
            for idx in sample_indices:
                sample = maps[idx]
                
                if not isinstance(sample, np.ndarray):
                    sample = np.array(sample)
                
                sample_max = int(sample.max())
                max_category = max(max_category, sample_max)
            
            print(f"âœ… {config.get('name', 'unknown')}: ìµœëŒ€ ì¹´í…Œê³ ë¦¬ = {max_category}")
                    
        except Exception as e:
            print(f"âš ï¸  {file_path} ê°ì§€ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"ğŸ“Š ê°ì§€ëœ ìµœëŒ€ ì¹´í…Œê³ ë¦¬: {max_category}")
    return max_category



def prepare_clean_data(data_configs, use_filter=True, filter_params=None, 
                       use_density_aware=False, use_region_aware=False):
    """
    ì—¬ëŸ¬ ì œí’ˆ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì™„ì „íˆ ì •ë¦¬ + í•„í„°ë§ + Multi-channel ë³€í™˜

    Args:
        data_configs: [{"path": "...", "name": "..."}, ...]
        use_filter: í•„í„°ë§ ì ìš© ì—¬ë¶€
        filter_params: í•„í„° íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        use_density_aware: Trueë©´ ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„° ì‚¬ìš©
        use_region_aware: Trueë©´ region-aware í•„í„° ì‚¬ìš©

    Returns:
        clean_maps: List of (n_categories+1, H, W) arrays
        clean_labels: List of labels
        info: List of filter info dicts
    """

    print("="*60)
    
    # 1. ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ ìë™ ê°ì§€ (ê°’ ê¸°ë°˜)
    n_categories = detect_n_categories(data_configs)
    n_channels = n_categories + 1  # 0: spatial pattern, 1~n: categories
    
    mode_str = "ë°€ë„ ê¸°ë°˜ ì ì‘í˜•" if use_density_aware else "ì¼ë°˜"
    print(f"ğŸ§¹ ë°ì´í„° ì™„ì „ ì •ë¦¬ ì‹œì‘" + (f" ({mode_str} í•„í„°ë§ í¬í•¨)" if use_filter else ""))
    print(f"ğŸ“Š Multi-channel format: {n_channels} channels (1 spatial + {n_categories} categories)")
    print("="*60)
    
    # í•„í„° ì´ˆê¸°í™”
    if use_filter:
        if use_density_aware:
            filter_obj = DensityAwareWaferMapFilter()
        else:
            if filter_params is None:
                filter_params = {
                    'min_component_size': 5,
                    'opening_kernel_size': 1,
                    'closing_kernel_size': 5,
                    'edge_preserve_strength': 0.9
                }
            filter_obj = WaferMapFilter(**filter_params)

    all_clean_maps = []
    all_clean_labels = []
    all_info = []

    for config in data_configs:
        file_path = config["path"]
        name = config.get("name", "unknown")

        print(f"\nğŸ“ {name} ë¡œë”© ì¤‘: {file_path}")

        if not os.path.exists(file_path):
            print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {file_path}")
            continue

        try:
            # ë°ì´í„° ë¡œë“œ
            data = np.load(file_path, allow_pickle=True)
            maps = data['maps']
            labels = data['ids'] if 'ids' in data else data['labels']

            print(f"   ì›ë³¸: {len(maps)}ê°œ")

            # ========== ë°°ì¹˜ ë³€í™˜ ==========
            # mapsë¥¼ numpy arrayë¡œ ë³€í™˜
            if not isinstance(maps, np.ndarray):
                maps = np.array(maps)
            
            # object dtype ì²˜ë¦¬
            if maps.dtype == object:
                # ê° ì›¨ì´í¼ì˜ shapeì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°œë³„ ì²˜ë¦¬ í•„ìš”
                maps_list = [np.array(m, dtype=np.float32) for m in maps]
            else:
                maps = maps.astype(np.float32)
                maps_list = None  # ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
            
            # ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥í•œ ê²½ìš° (ëª¨ë“  ì›¨ì´í¼ shape ë™ì¼)
            if maps_list is None:
                # (n, H, W) â†’ (n, 13, H, W) ë°°ì¹˜ ë³€í™˜
                multi_channel_maps = convert_to_multichannel(maps, n_categories=n_categories)
                
                print(f"   ë°°ì¹˜ ë³€í™˜ ì™„ë£Œ: {multi_channel_maps.shape}")
                
                # ê°œë³„ í•„í„°ë§ ë° ê²€ì¦
                for i in range(len(multi_channel_maps)):
                    wm = multi_channel_maps[i]  # (13, H, W)
                    label = labels[i]
                    
                    # NaN, Inf ì²˜ë¦¬
                    if np.any(np.isnan(wm)) or np.any(np.isinf(wm)):
                        wm = np.nan_to_num(wm, nan=0.0, posinf=1.0, neginf=0.0)
                    
                    # í•„í„°ë§ ì ìš© (channel 0ì—ë§Œ)
                    info = None
                    if use_filter and wm[0].sum() > 0:
                        if use_density_aware:
                            wm[0], info = filter_obj.filter_single_map(wm[0])
                        else:
                            wm[0] = filter_obj.filter_single_map(wm[0])
                    
                    all_clean_maps.append(wm)
                    all_clean_labels.append(label)
                    all_info.append(info)
            
            else:
                # shapeì´ ë‹¤ë¥¸ ê²½ìš° ê°œë³„ ì²˜ë¦¬
                print(f"   âš ï¸  ì›¨ì´í¼ í¬ê¸°ê°€ ë‹¤ë¦„ - ê°œë³„ ì²˜ë¦¬")
                
                for i, (wm, label) in enumerate(zip(maps_list, labels)):
                    try:
                        # 2D ê²€ì¦
                        if len(wm.shape) != 2 or wm.shape[0] == 0 or wm.shape[1] == 0:
                            continue
                        
                        # ë‹¨ì¼ ì›¨ì´í¼ ë³€í™˜
                        multi_wm = convert_to_multichannel(wm, n_categories=n_categories)
                        
                        # NaN, Inf ì²˜ë¦¬
                        if np.any(np.isnan(multi_wm)) or np.any(np.isinf(multi_wm)):
                            multi_wm = np.nan_to_num(multi_wm, nan=0.0, posinf=1.0, neginf=0.0)
                        
                        # í•„í„°ë§ ì ìš© (channel 0ì—ë§Œ)
                        info = None
                        if use_filter and multi_wm[0].sum() > 0:                            
                            if use_density_aware:
                                multi_wm[0], info = filter_obj.filter_single_map(multi_wm[0])
                            else:
                                multi_wm[0] = filter_obj.filter_single_map(multi_wm[0])
                        
                        all_clean_maps.append(multi_wm)
                        all_clean_labels.append(label)
                        all_info.append(info)
                        
                    except Exception as e:
                        continue
            
            print(f"   ì •ë¦¬ë¨: {len(all_clean_maps)}ê°œ")

        except Exception as e:
            print(f"âŒ {name} ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\n{'='*60}")
    print(f"âœ… ì „ì²´ ì •ë¦¬ ì™„ë£Œ: {len(all_clean_maps)}ê°œ")
    print(f"   Shape per sample: ({n_channels}, H, W)")
    print(f"{'='*60}")
    
    return all_clean_maps, all_clean_labels, all_info


class MultiSizeWaferDataset(Dataset):
    """ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” Dataset"""

    def __init__(self, wafer_maps, labels, target_size=(128, 128), 
                 use_filter=False, filter_on_the_fly=False, filter_params=None,
                 use_density_aware=False, is_training=False, use_augmentation=False):
        """
        Args:
            wafer_maps: ì›¨ì´í¼ë§µ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            target_size: ë¦¬ì‚¬ì´ì¦ˆ íƒ€ê²Ÿ í¬ê¸°
            use_filter: í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
            filter_on_the_fly: Trueë©´ __getitem__ ì‹œë§ˆë‹¤ í•„í„°ë§ (ëŠë¦¼, ë©”ëª¨ë¦¬ ì ˆì•½)
                              Falseë©´ ì´ˆê¸°í™” ì‹œ ëª¨ë‘ í•„í„°ë§ (ë¹ ë¦„, ë©”ëª¨ë¦¬ ì‚¬ìš©)
            filter_params: í•„í„° íŒŒë¼ë¯¸í„°
            use_density_aware: Trueë©´ ë°€ë„ ê¸°ë°˜ ì ì‘í˜• í•„í„° ì‚¬ìš© (ê¶Œì¥!)
        """
        self.wafer_maps = []
        self.labels = []
        self.original_indices = []
        self.target_size = target_size
        self.use_filter = use_filter
        self.filter_on_the_fly = filter_on_the_fly
        self.use_density_aware = use_density_aware
        self.is_training = is_training
        self.use_augmentation = use_augmentation

        # í•„í„° ì´ˆê¸°í™”
        if use_filter:
            if use_density_aware:
                self.filter_obj = DensityAwareWaferMapFilter()
            else:
                if filter_params is None:
                    filter_params = {
                        'min_component_size': 5,
                        'opening_kernel_size': 1,
                        'closing_kernel_size': 5,
                        'edge_preserve_strength': 0.9
                    }
                self.filter_obj = WaferMapFilter(**filter_params)
        
        print(f"ğŸ›¡ï¸  Dataset ìƒì„± ì¤‘...")
        if use_filter and not filter_on_the_fly:
            print(f"   ì‚¬ì „ í•„í„°ë§ ì ìš© ì¤‘...")

        for idx, (wm, label) in enumerate(zip(wafer_maps, labels)):
            # ğŸ”´ Shape ê²€ì¦ ìˆ˜ì •: (C, H, W) í˜•ì‹
            if (isinstance(wm, np.ndarray) and
                wm.dtype == np.float32 and
                len(wm.shape) == 3 and      # (C, H, W)
                wm.shape[0] > 0 and         # C > 0
                wm.shape[1] > 0 and         # H > 0
                wm.shape[2] > 0):           # W > 0

                # ì‚¬ì „ í•„í„°ë§ (filter_on_the_fly=Falseì¸ ê²½ìš°)
                # Channel 0ì—ë§Œ ì ìš©
                if use_filter and not filter_on_the_fly:
                    if wm[0].sum() > 0:
                        original_defects = wm[0].sum()
                        
                        if use_density_aware:
                            wm[0], info = self.filter_obj.filter_single_map(wm[0])
                        else:
                            wm[0] = self.filter_obj.filter_single_map(wm[0])
                        
                        # ë„ˆë¬´ ë§ì´ ì œê±°ë˜ë©´ ìŠ¤í‚µ
                        if wm[0].sum() < original_defects * 0.2:
                            continue
                
                self.wafer_maps.append(wm)
                self.labels.append(label)
                self.original_indices.append(idx)

        print(f"   ìµœì¢… Dataset: {len(self.wafer_maps)}ê°œ")

    def __len__(self):
        return len(self.wafer_maps)

    def __getitem__(self, idx):
        wafer_map = self.wafer_maps[idx]  # (C, H, W) - already multi-channel
        label = self.labels[idx]
        original_idx = self.original_indices[idx]

        # On-the-fly í•„í„°ë§ (filter_on_the_fly=Trueì¸ ê²½ìš°)
        # Note: ì´ë¯¸ prepare_clean_dataì—ì„œ í•„í„°ë§ í–ˆìœ¼ë¯€ë¡œ ë³´í†µì€ skip
        if self.use_filter and self.filter_on_the_fly:
            # Channel 0ì—ë§Œ í•„í„°ë§ ì ìš©
            if wafer_map[0].sum() > 0:
                if self.use_density_aware:
                    wafer_map[0], _ = self.filter_obj.filter_single_map(wafer_map[0])
                else:
                    wafer_map[0] = self.filter_obj.filter_single_map(wafer_map[0])

        # ì•ˆì „í•œ ì „ì²˜ë¦¬
        # wafer_map: (C, H, W) numpy array â†’ (C, target_H, target_W) tensor
        tensor = torch.tensor(wafer_map, dtype=torch.float32)  # (C, H, W)
        
        # Resize
        # F.interpolate expects (B, C, H, W), so add batch dim
        tensor_4d = tensor.unsqueeze(0)  # (1, C, H, W)
        resized = F.interpolate(tensor_4d, size=self.target_size, mode='bilinear', align_corners=False)
        resized = resized.squeeze(0)  # (C, target_H, target_W)

        # ğŸ”´ Augmentation ì ìš© (training ì‹œì—ë§Œ!)
        if self.is_training and self.use_augmentation:
            resized_aug = self._apply_augmentation(resized)
        else:
            resized_aug = None

        return resized, resized_aug, label, original_idx
    
    def _apply_augmentation(self, tensor):
        """
        íšŒì „ ë¶ˆë³€ì„±ì„ ìœ„í•œ Augmentation
        D4 Dihedral groupì˜ 8ê°€ì§€ ë³€í™˜ ì¤‘ í•˜ë‚˜ë¥¼ ê· ë“±í•˜ê²Œ ì„ íƒ
        
        Args:
            tensor: (C, H, W) - multi-channel
        
        Returns:
            tensor: (C, H, W) - augmented
        """
        # 8ê°€ì§€ ë³€í™˜ ì¤‘ í•˜ë‚˜ë¥¼ ê· ë“±í•˜ê²Œ ì„ íƒ
        transform_id = torch.randint(0, 8, (1,)).item()
        
        if transform_id == 0:
            return tensor  # Identity (ë³€í™˜ ì—†ìŒ)
        elif transform_id == 1:
            return torch.rot90(tensor, 1, dims=[1, 2])  # 90ë„ íšŒì „
        elif transform_id == 2:
            return torch.rot90(tensor, 2, dims=[1, 2])  # 180ë„ íšŒì „
        elif transform_id == 3:
            return torch.rot90(tensor, 3, dims=[1, 2])  # 270ë„ íšŒì „
        elif transform_id == 4:
            return torch.flip(tensor, dims=[2])  # ì¢Œìš° ë°˜ì „
        elif transform_id == 5:
            return torch.flip(tensor, dims=[1])  # ìƒí•˜ ë°˜ì „
        elif transform_id == 6:
            # 90ë„ íšŒì „ + ì¢Œìš° ë°˜ì „ (ëŒ€ê°ì„  ëŒ€ì¹­)
            return torch.flip(torch.rot90(tensor, 1, dims=[1, 2]), dims=[2])
        elif transform_id == 7:
            # 90ë„ íšŒì „ + ìƒí•˜ ë°˜ì „ (ë‹¤ë¥¸ ëŒ€ê°ì„  ëŒ€ì¹­)
            return torch.flip(torch.rot90(tensor, 1, dims=[1, 2]), dims=[1])


def collate_fn(batch):
    """ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•œ collate í•¨ìˆ˜
    + ë¬¸ì œ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ í•„í„°ë§í•˜ëŠ” collate í•¨ìˆ˜
    + ë¹ˆ ë§µ(ëª¨ë‘ 0)ë„ ì œê±°
    + Multi-channel ì§€ì›"""

    safe_data = []
    safe_data_aug = []
    safe_labels = []
    safe_indices = []
    has_aug = True

    for data, data_aug, label, original_idx in batch:
        try:
            # Shape ê²€ì¦: (C, H, W)
            if (isinstance(data, torch.Tensor) and
                data.dtype == torch.float32 and
                len(data.shape) == 3 and  # (C, H, W)
                data.shape[0] > 0 and     # C > 0
                data.shape[1] > 0 and     # H > 0
                data.shape[2] > 0):       # W > 0

                safe_data.append(data)
                safe_data_aug.append(data_aug)
                safe_labels.append(label)
                safe_indices.append(original_idx)

                # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ augmentation ì—¬ë¶€ íŒë‹¨
                if len(safe_data) == 1:
                    has_aug = (data_aug is not None)

        except:
            continue

    if len(safe_data) == 0:
        # ëª¨ë“  ìƒ˜í”Œì´ ë¬¸ì œì¸ ê²½ìš° ë”ë¯¸ ë°°ì¹˜ ë°˜í™˜
        # Multi-channel dummy
        dummy = torch.zeros((1, 13, 128, 128), dtype=torch.float32)  # 11 channels
        return dummy, None, ["dummy"], [0]

    batch_data = torch.stack(safe_data)
    
    if has_aug:
        batch_data_aug = torch.stack(safe_data_aug)
    else:
        batch_data_aug = None

    return batch_data, batch_data_aug, safe_labels, safe_indices



def create_dataloaders(wafer_maps, labels, batch_size=64, target_size=(128, 128), test_size=0.2, 
                        use_filter=True, filter_on_the_fly=False, filter_params=None, 
                        use_density_aware=False, use_augmentation=False):
    
    print("\nğŸ”§ ì•ˆì „í•œ DataLoader ìƒì„±")
    print("="*40)

    if use_filter:
        if use_density_aware:
            mode = "Density-Aware (ë°€ë„ ê¸°ë°˜ ì ì‘í˜•)"
        elif filter_on_the_fly:
            mode = "On-the-fly"
    else:
        mode = "Pre-filtering"
    print(f"   í•„í„°ë§ ëª¨ë“œ: {mode}")

    # ğŸ”¹ train/valid ë¶„í• ì„ ë¨¼ì € ìˆ˜í–‰
    train_indices, valid_indices = train_test_split(
        range(len(wafer_maps)), test_size=test_size, random_state=42
    )
    
    # ğŸ”¹ ë¶„í• ëœ ë°ì´í„°ë¡œ train/valid ë°ì´í„° ìƒì„±
    train_maps = [wafer_maps[i] for i in train_indices]
    train_labels = [labels[i] for i in train_indices]
    
    valid_maps = [wafer_maps[i] for i in valid_indices]
    valid_labels = [labels[i] for i in valid_indices]

    # ğŸ”¹ ë³„ë„ì˜ dataset ê°ì²´ ìƒì„±
    train_dataset = MultiSizeWaferDataset(
        train_maps, train_labels, 
        target_size=target_size,
        use_filter=use_filter,
        filter_on_the_fly=filter_on_the_fly,
        filter_params=filter_params,
        use_density_aware=use_density_aware,
        is_training=True,  # ğŸ”¹ trainì€ True
        use_augmentation=use_augmentation
    )
    
    valid_dataset = MultiSizeWaferDataset(
        valid_maps, valid_labels, 
        target_size=target_size,
        use_filter=use_filter,
        filter_on_the_fly=filter_on_the_fly,
        filter_params=filter_params,
        use_density_aware=use_density_aware,
        is_training=False,  # ğŸ”¹ validì€ False
        use_augmentation=False  # ğŸ”¹ í•­ìƒ False
    )

    print(f"   Train: {len(train_dataset)}ê°œ (Augmentation: {use_augmentation})")
    print(f"   Valid: {len(valid_dataset)}ê°œ (Augmentation: False Fixed)")

    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=False,
        drop_last=False
    )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=False,
        drop_last=False
    )

    print(f"   Train ë°°ì¹˜: {len(train_loader)}ê°œ / Valid ë°°ì¹˜: {len(valid_loader)}ê°œ")

    return train_loader, valid_loader